{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1TjLH8cnXMv_CjuistNr0UNFfsT33DxTz","authorship_tag":"ABX9TyPa4St+iYoBtDotnnIvT935"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BM8U5u-3Lmak","executionInfo":{"status":"ok","timestamp":1703102653786,"user_tz":-480,"elapsed":228346,"user":{"displayName":"Zhitao Wan","userId":"02799801938471273149"}},"outputId":"d749becc-4841-487a-8815-650779918c8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 110554882.27it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./mnist/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 17352197.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 158316349.16it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 9602081.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist/MNIST/raw\n","\n","[epoch 1, 0.05%] loss: 2.302\n","[epoch 1, 5.39%] loss: 175.183\n","[epoch 1, 10.72%] loss: 157.970\n","[epoch 1, 16.05%] loss: 154.773\n","[epoch 1, 21.39%] loss: 155.781\n","[epoch 1, 26.72%] loss: 154.937\n","[epoch 1, 32.05%] loss: 154.259\n","[epoch 1, 37.39%] loss: 153.178\n","[epoch 1, 42.72%] loss: 152.849\n","[epoch 1, 48.05%] loss: 152.493\n","[epoch 1, 53.39%] loss: 151.942\n","[epoch 1, 58.72%] loss: 152.960\n","[epoch 1, 64.05%] loss: 152.619\n","[epoch 1, 69.39%] loss: 152.015\n","[epoch 1, 74.72%] loss: 152.345\n","[epoch 1, 80.05%] loss: 151.109\n","[epoch 1, 85.39%] loss: 152.355\n","[epoch 1, 90.72%] loss: 151.746\n","[epoch 1, 96.05%] loss: 151.581\n","[epoch 2, 0.05%] loss: 1.497\n","[epoch 2, 5.39%] loss: 150.204\n","[epoch 2, 10.72%] loss: 150.440\n","[epoch 2, 16.05%] loss: 150.722\n","[epoch 2, 21.39%] loss: 151.304\n","[epoch 2, 26.72%] loss: 151.456\n","[epoch 2, 32.05%] loss: 150.535\n","[epoch 2, 37.39%] loss: 150.249\n","[epoch 2, 42.72%] loss: 150.751\n","[epoch 2, 48.05%] loss: 150.435\n","[epoch 2, 53.39%] loss: 150.178\n","[epoch 2, 58.72%] loss: 150.752\n","[epoch 2, 64.05%] loss: 150.826\n","[epoch 2, 69.39%] loss: 150.530\n","[epoch 2, 74.72%] loss: 150.932\n","[epoch 2, 80.05%] loss: 149.304\n","[epoch 2, 85.39%] loss: 150.363\n","[epoch 2, 90.72%] loss: 150.459\n","[epoch 2, 96.05%] loss: 150.627\n","[epoch 3, 0.05%] loss: 1.492\n","[epoch 3, 5.39%] loss: 149.781\n","[epoch 3, 10.72%] loss: 150.398\n","[epoch 3, 16.05%] loss: 150.497\n","[epoch 3, 21.39%] loss: 149.653\n","[epoch 3, 26.72%] loss: 150.773\n","[epoch 3, 32.05%] loss: 150.123\n","[epoch 3, 37.39%] loss: 149.909\n","[epoch 3, 42.72%] loss: 150.000\n","[epoch 3, 48.05%] loss: 150.006\n","[epoch 3, 53.39%] loss: 150.112\n","[epoch 3, 58.72%] loss: 149.502\n","[epoch 3, 64.05%] loss: 148.977\n","[epoch 3, 69.39%] loss: 149.742\n","[epoch 3, 74.72%] loss: 149.833\n","[epoch 3, 80.05%] loss: 149.954\n","[epoch 3, 85.39%] loss: 150.061\n","[epoch 3, 90.72%] loss: 150.152\n","[epoch 3, 96.05%] loss: 150.239\n","[epoch 4, 0.05%] loss: 1.524\n","[epoch 4, 5.39%] loss: 149.839\n","[epoch 4, 10.72%] loss: 149.345\n","[epoch 4, 16.05%] loss: 149.359\n","[epoch 4, 21.39%] loss: 149.567\n","[epoch 4, 26.72%] loss: 149.663\n","[epoch 4, 32.05%] loss: 149.867\n","[epoch 4, 37.39%] loss: 149.099\n","[epoch 4, 42.72%] loss: 149.682\n","[epoch 4, 48.05%] loss: 149.722\n","[epoch 4, 53.39%] loss: 149.143\n","[epoch 4, 58.72%] loss: 149.163\n","[epoch 4, 64.05%] loss: 149.800\n","[epoch 4, 69.39%] loss: 149.354\n","[epoch 4, 74.72%] loss: 149.581\n","[epoch 4, 80.05%] loss: 149.584\n","[epoch 4, 85.39%] loss: 149.512\n","[epoch 4, 90.72%] loss: 149.584\n","[epoch 4, 96.05%] loss: 149.436\n","[epoch 5, 0.05%] loss: 1.491\n","[epoch 5, 5.39%] loss: 149.053\n","[epoch 5, 10.72%] loss: 149.441\n","[epoch 5, 16.05%] loss: 149.471\n","[epoch 5, 21.39%] loss: 148.982\n","[epoch 5, 26.72%] loss: 149.121\n","[epoch 5, 32.05%] loss: 149.293\n","[epoch 5, 37.39%] loss: 149.146\n","[epoch 5, 42.72%] loss: 149.171\n","[epoch 5, 48.05%] loss: 149.643\n","[epoch 5, 53.39%] loss: 148.924\n","[epoch 5, 58.72%] loss: 149.608\n","[epoch 5, 64.05%] loss: 149.408\n","[epoch 5, 69.39%] loss: 148.873\n","[epoch 5, 74.72%] loss: 149.449\n","[epoch 5, 80.05%] loss: 149.402\n","[epoch 5, 85.39%] loss: 148.688\n","[epoch 5, 90.72%] loss: 149.333\n","[epoch 5, 96.05%] loss: 149.572\n","Finished Training\n","Evaluating ...\n","Accuracy of the network on the test images: 96 %\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","class Model:\n","    def __init__(self, net, cost, optimist):\n","        self.net = net\n","        self.cost = self.create_cost(cost)\n","        self.optimizer = self.create_optimizer(optimist)\n","\n","    def create_cost(self, cost):\n","        support_cost = {\n","            'CROSS_ENTROPY': nn.CrossEntropyLoss(),\n","            'MSE': nn.MSELoss()\n","        }\n","\n","        return support_cost[cost]\n","\n","    def create_optimizer(self, optimist, **rests):\n","        support_optim = {\n","            'SGD': optim.SGD(self.net.parameters(), lr=0.1, **rests),\n","            'ADAM': optim.Adam(self.net.parameters(), lr=0.01, **rests),\n","            'RMSP':optim.RMSprop(self.net.parameters(), lr=0.001, **rests)\n","        }\n","\n","        return support_optim[optimist]\n","\n","    def train(self, train_loader, epoches=5):\n","        for epoch in range(epoches):\n","            running_loss = 0.0\n","            for i, data in enumerate(train_loader, 0): # start = 0\n","                inputs, labels = data\n","\n","                self.optimizer.zero_grad() # Resets the gradients of all optimized tensor, set_to_none=True means instead of setting to zero, set the grads to None.\n","\n","                # forward + backward + optimize\n","                outputs = self.net(inputs)\n","                loss = self.cost(outputs, labels)\n","                loss.backward() # Compute gradients of the parameters respect to the loss\n","                self.optimizer.step() # updates the parameters\n","\n","                running_loss += loss.item()\n","                if i % 100 == 0:\n","                    print('[epoch %d, %.2f%%] loss: %.3f' %\n","                          (epoch + 1, (i + 1)*100/len(train_loader), running_loss))\n","                    running_loss = 0.0\n","\n","        print('Finished Training')\n","\n","    def evaluate(self, test_loader):\n","        print('Evaluating ...')\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():  # no grad when test and predict\n","            for data in test_loader:\n","                images, labels = data\n","\n","                outputs = self.net(images)\n","                predicted = torch.argmax(outputs, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","        print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))\n","\n","def mnist_load_data():\n","    transform = transforms.Compose(\n","        [transforms.ToTensor(), # Convert a PIL Image or ndarray to tensor and scale the values [0.0, 1.0] accordingly\n","         transforms.Normalize([0], [1])]) # Normalize a tensor image with mean and standard deviation\n","\n","    trainset = torchvision.datasets.MNIST(root='./mnist', train=True,\n","                                            download=True, transform=transform)\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n","                                              shuffle=True, num_workers=2)\n","\n","    testset = torchvision.datasets.MNIST(root='./mnist', train=False,\n","                                           download=True, transform=transform)\n","    testloader = torch.utils.data.DataLoader(testset, batch_size=32,shuffle=True, num_workers=2)\n","    return trainloader, testloader\n","\n","\n","class MnistNet(torch.nn.Module):\n","    def __init__(self):\n","        super(MnistNet, self).__init__()\n","        self.fc1 = torch.nn.Linear(28*28, 512)\n","        self.fc2 = torch.nn.Linear(512, 512)\n","        self.fc3 = torch.nn.Linear(512, 10)\n","\n","    def forward(self, x):\n","        x = x.view(-1, 28*28)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = F.softmax(self.fc3(x), dim=1)\n","        return x\n","\n","if __name__ == '__main__':\n","    net = MnistNet()\n","    model = Model(net, 'CROSS_ENTROPY', 'RMSP')\n","    train_loader, test_loader = mnist_load_data()\n","    model.train(train_loader)\n","    model.evaluate(test_loader)"]}]}