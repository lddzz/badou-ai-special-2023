1、Dropout可以作为训练深度神经网络的一种trick供选择。
    在每个训练批次中，通过忽略一半数量的特征检测器（让一半的隐层节点值为0），可以明显地减少过拟合现象。
    这种方式可以减少特征检测器（隐层节点）间的相互作用，检测器相互作用是指某些检测器依赖其他检测器才能发挥作用。

     作用：保证稀疏性

     Dropout说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p（伯努利分布）停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征，

      原文链接：https://blog.csdn.net/PETERPARKERRR/article/details/121888093
2、BatchNormalization
    通过规范化的手段,将越来越偏的分布拉回到标准化的分布,使得激活函数的输入值落在激活函数对输入比较敏感的区域,从而使梯度变大,加快学习收敛速度,避免梯度消失的问题
    好处：使得数据能够分布在激活函数的敏感区域，敏感区域即为梯度较大的区域，因此在反向传播的时候能够较快反馈误差传播